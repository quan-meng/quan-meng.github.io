<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>LT3SD: Latent Trees for 3D Scene Diffusion</title>
    <meta name="author" content="Quan Meng" />
    <meta name="description" content="LT3SD: Latent Trees for 3D Scene Diffusion" />
    <meta name="keywords" content="3D Scene Generation, Diffusion Model" />
    
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:400,500,700|Roboto+Slab:400,500,700|Material+Icons">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark" />
    <link rel="shortcut icon" href="/assets/img/favicon.ico"/>
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://quan-meng.github.io/projects/lt3sd">
    
    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    
    <style>
      .embed-responsive-3by1 {
        padding-top: 35%;
      }
      .content-block {
        width: 90%;            /* Increased from 80% for better mobile use */
        max-width: 800px;
        margin: 0 auto;
        padding: 0 15px;       /* Added padding for mobile */
      }
      
      /* Added mobile-specific styles */
      @media (max-width: 768px) {
        .content-block {
          width: 95%;
        }
        
        .list-inline-item {
          display: block;      /* Stack items vertically on mobile */
          margin: 8px 0;
          text-align: center;
        }
        
        h1.post-title {
          font-size: 1.8rem;   /* Smaller title on mobile */
          padding: 0 10px;
        }
        
        h4 {
          font-size: 1.2rem;   /* Smaller author names on mobile */
        }
        
        .btn {
          width: 100%;         /* Full-width buttons on mobile */
          margin: 5px 0;
        }
      }
    </style>
</head>

<body class="sticky-bottom-footer">
    <div class="container mt-5">
        <div class="post">
            <h1 class="post-title text-center">LT3SD: Latent Trees for 3D Scene Diffusion</h1>
            <div class="d-flex justify-content-center">
                <ul class="list-inline">
                    <li class="list-inline-item px-3 pt-3">
                        <h4><a href="https://quan-meng.github.io" target="_blank" rel="noopener noreferrer">Quan Meng</a></h4>
                    </li>
                    <li class="list-inline-item px-3 pt-3">
                        <h4><a href="https://craigleili.github.io" target="_blank" rel="noopener noreferrer">Lei Li</a></h4>
                    </li>
                    <li class="list-inline-item px-3 pt-3">
                        <h4><a href="https://www.niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nie√üner</a></h4>
                    </li>
                    <li class="list-inline-item px-3 pt-3">
                        <h4><a href="https://www.3dunderstanding.org/team.html" target="_blank" rel="noopener noreferrer">Angela Dai</a></h4>
                    </li>
                </ul>
            </div>
            <div class="d-flex justify-content-center">
                <ul class="list-inline">
                    <li class="list-inline-item">
                        <h5>Technical University of Munich</h5>
                    </li>
                </ul>
            </div>
            
            <div class="d-flex justify-content-center">
                <ul class="list-inline">
                    <li class="list-inline-item">
                        <h4>CVPR 2025</h4>
                    </li>
                </ul>
            </div>

            <div class="d-flex justify-content-center">
                <ul class="list-inline">
                    <li class="list-inline-item px-2 py-2">  <!-- Adjusted padding -->
                        <a href="http://arxiv.org/pdf/2409.08215" target="_blank" rel="noopener noreferrer">
                            <span class="btn btn-outline-dark btn-sm waves-effect waves-light"><i class="fas fa-file-pdf"></i> Paper</span>
                        </a>
                    </li>
                    <li class="list-inline-item px-2 py-2">  <!-- Adjusted padding -->
                        <a href="https://youtu.be/AJ5sG9VyjGA" target="_blank" rel="noopener noreferrer">
                            <span class="btn btn-outline-dark btn-sm waves-effect waves-light"><i class="fab fa-youtube"></i> Video</span>
                        </a>
                    </li>
                    <li class="list-inline-item px-2 py-2">  <!-- Adjusted padding -->
                        <a href="https://github.com/quan-meng/lt3sd" target="_blank" rel="noopener noreferrer">
                            <span class="btn btn-outline-dark btn-sm waves-effect waves-light"><i class="fab fa-github"></i> Code</span>
                        </a>
                    </li>
                </ul>
            </div>

            <div class="embed-responsive embed-responsive-16by9">
                <video class="embed-responsive-item" muted autoplay loop>
                    <source src="../../assets/lt3sd/fly_video.mp4" type="video/mp4" />
                </video>
            </div>

            <h2 class="text-center">Abstract</h2>

            <p class="content-block">
                We present <strong>LT3SD</strong>, a novel latent diffusion model for large-scale 3D scene generation. 
                Recent advances in diffusion models have shown impressive results in 3D object generation, 
                but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, 
                we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. 
                We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. 
                To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes 
                through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy 
                and benefits of <strong>LT3SD</strong> for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.

            <div class="embed-responsive embed-responsive-16by9">
                <video class="embed-responsive-item" muted autoplay loop>
                    <source src="../../assets/lt3sd/intermediate_video.mp4" type="video/mp4" />
                </video>
            </div>
            <p class="content-block">
                <strong>LT3SD</strong> enables high-fidelity generation of infinite 3D environments in a patch-by-patch and coarse-to-fine fashion.
                Samples of generated infinite 3D scene mesh can be downloaded <a href="https://1drv.ms/f/c/e762fb0a44e578db/Euk-_7-eXrVKl4TwQ7lTiUkBshMygyB4U7QLTkPp2jkJMQ?e=wGwcj4" target="_blank" rel="noopener noreferrer">here</a>.
            </p>
                
            <h2 class="text-center pt-3">Video</h2>
            <div class="container content-block">
                <div class="embed-responsive embed-responsive-16by9">
                    <iframe class="embed-responsive-item" src="https://www.youtube.com/embed/AJ5sG9VyjGA?si=aeYxajefHDz8PKwO" allowfullscreen=""></iframe>
                </div>
            </div>

            <h2 class="text-center pt-5">How It Works</h2>
            <div class='content-block'>
                <div class="row">
                    <div class="col-sm mt-3 mt-md-0">
                        <figure>
                            <picture>
                                <img class="img-fluid rounded z-depth-1" src="../../assets/lt3sd/pipeline.png" title="Method">
                            </picture>
                        </figure>
                    </div>
                </div>
            </div>

            <p class="content-block">
                We formulate 3D scene generation as a patch-based latent diffusion process. 
                <strong>Left</strong>: To characterize complex scene geometry, we encode 3D scenes in a novel latent tree representation, where each scene resolution level is decomposed into 
                a TUDF grid and a latent feature grid. 
                <strong>Top Right</strong>: During latent tree training, the encoder encodes a patch from the scene grid at resolution level i+1 to a coarser TUDF patch and a latent feature patch at level i. 
                The decoder then reconstructs the scene patch based on the factorized grids. 
                <strong>Bottom Right</strong>: During generation, the diffusion model learns to generate a latent 
                feature patch conditioned on a TUDF patch within the same level. 
                Our method enables arbitrary-sized 3D scene generation at inference time by synthesizing scenes in a coarse-to-fine hierarchy and a patch-by-patch fashion.
            </p>

            <h2 class="text-center pt-3">Comparisons</h2>
            <div class="content-block">
                <img class="img-fluid rounded" src="../../assets/lt3sd/comparison.png" title="Comparisons">
            </div>
            
            <p class="content-block">
                We compare unconditional 3D scene generation with diverse 3D diffusion methods PVD, NFD, and BlockFusion. 
                All methods were trained on the house level of the 3D-FRONT dataset. Our latent tree-based 3D scene diffusion approach synthesizes cleaner surfaces with 
                more geometric details and captures diverse furniture objects.
            </p>

            <h2 class="text-center pt-3">Outdoor Scene Generation</h2>
            <div class="content-block text-center">
                <img class="img-fluid rounded mx-auto d-block" src="../../assets/lt3sd/outdoor.png" title="Outdoor Scene Generation" style="height: 50%; width: 50%;">
            </div>

            <p class="content-block">
                Our approach leverages a unified voxel-based representation for 3D scenes that generalizes beyond indoor environments. This enables seamless generation of diverse outdoor scenes, demonstrating the flexibility and robustness of our method across different types of 3D environments.
            </p>

            <h2 class="text-center pt-3">Citation</h2>
            <div class="content-block">
                <div class="bibtex">
                    <figure class="highlight">
                        <pre><code class="language-bibtex" data-lang="bibtex">
@article{meng2024lt3sdlatenttrees3d,
    title={LT3SD: Latent Trees for 3D Scene Diffusion}, 
    author={Quan Meng and Lei Li and Matthias Nie√üner and Angela Dai},
    journal={arXiv preprint arXiv:2409.08215},
    year={2024}
}
                        </code></pre>
                    </figure>
                </div>
            </div>

            <h2 class="text-center pt-3">Acknowledgements</h2>
            <p class="content-block">
                This work was supported by the ERC Starting Grant SpatialSem (101076253). Matthias Niessner was supported by the ERC Starting Grant Scan2CAD (804724).
            </p>
        </div>
    </div>

</body>

</html>
